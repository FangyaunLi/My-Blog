{
  "hash": "b4631d440511ce8c725b0a4c81425287",
  "result": {
    "markdown": "---\ntitle: 实现简单线性回归\ncategories: [R, econometrics]\ndate: \"2024-03-25\"\nnumber-depth: 2\ntoc-depth: 3\n---\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntheme_set(hrbrthemes::theme_ipsum_rc())\n```\n:::\n\n\n\n准备花几期时间，自己写代码，实现常用的计量方法。\n\n生成模拟数据\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(111)\nx <- 1:4\nnames(x) <- LETTERS[1:4]\ndf <- tibble(\n  x1 = runif(1000, -10, 10),\n  x2 = sample(1:20, 1000, replace = TRUE),\n  x3 = sample(LETTERS[1:4], 1000, replace = TRUE),\n  error = rnorm(1000, sd = 1000),\n  y = 1 + 2 * x1 + 3 * x2 + x[x3]\n)\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,000 × 5\n       x1    x2 x3     error     y\n    <dbl> <int> <chr>  <dbl> <dbl>\n 1  1.86     11 A      285.  38.7 \n 2  4.53      5 B      176.  27.1 \n 3 -2.59     11 A     -610.  29.8 \n 4  0.298     9 A       92.1 29.6 \n 5 -2.45     20 D      611.  60.1 \n 6 -1.63     15 B      170.  44.7 \n 7 -9.79      6 C     -104.   2.43\n 8  0.646    18 A     -253.  57.3 \n 9 -1.36     10 B     -169.  30.3 \n10 -8.13      6 B     -509.   4.75\n# ℹ 990 more rows\n```\n:::\n:::\n\n\n# 步骤拆解\n\n### step 1: 根据formula生成矩阵\n\n因为`formula`是R里面一个特殊的`class`，他的具体机制我没弄清楚，所以这一步用R中的`model.matrix.lm()`实现\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nformula <- y ~ x1 + x2 + x3\ndf_model <- model.frame(formula, df)\nX <- model.matrix.lm(df_model)\nhead(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept)         x1 x2 x3B x3C x3D\n1           1  1.8596257 11   0   0   0\n2           1  4.5296224  5   1   0   0\n3           1 -2.5915599 11   0   0   0\n4           1  0.2984766  9   0   0   0\n5           1 -2.4467357 20   0   0   1\n6           1 -1.6332535 15   1   0   0\n```\n:::\n\n```{.r .cell-code}\nY <- model.response(df_model)\nhead(Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1        2        3        4        5        6 \n38.71925 27.05924 29.81688 29.59695 60.10653 44.73349 \n```\n:::\n:::\n\n\n### step 2: 得到回归系数\n\n假设对于总体数据来说：\n\n$$\n\\mathbf Y = \\mathbf X \\boldsymbol \\beta + \\boldsymbol \\epsilon\n$$\n\n\n但是我们得到的拟合方程是：\n\n$$\n\\hat{\\mathbf Y} = \\mathbf X \\hat{\\boldsymbol \\beta}\n$$\n\n\n\n\n且\n\n$$\n\\begin{align}\n\\boldsymbol e &= \\mathbf Y - \\hat{\\mathbf Y} \\\\\n&= \\mathbf Y - \\mathbf X \\hat{\\boldsymbol \\beta}\n\\end{align}\n$$\n\n要想使$\\mathbf Y - \\hat{\\mathbf Y}$最小，则$\\boldsymbol e$与$\\mathbf X$正交，即\n\n\n$$\n\\begin{align}\n&\\mathbf {X^T (Y - X\\hat{\\beta})} = \\mathbf {0} \\\\\n&\\mathbf {\\hat{\\beta}} = \\mathbf {(X^T X)^{-1}X^{T} Y} \n\\end{align}\n$$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_beta <- function(X, Y) {\n  solve(crossprod(X, X)) %*% crossprod(X, Y)\n}\nbeta <- get_beta(X, Y)\nbeta\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            [,1]\n(Intercept)    2\nx1             2\nx2             3\nx3B            1\nx3C            2\nx3D            3\n```\n:::\n:::\n\n\n### step 3: 求出拟合值和误差\n\n$$\n\\begin{align}\n\\mathbf{\\hat{Y}} &= \\mathbf{X\\hat{\\beta}} \\\\\n\\mathbf{e} &= \\mathbf{ Y - \\hat{Y}}\n\\end{align}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_fit <- function(X, beta) {\n  X %*% beta\n}\n\nY_hat <- get_fit(X, beta)\nhead(Y_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      [,1]\n1 38.71925\n2 27.05924\n3 29.81688\n4 29.59695\n5 60.10653\n6 44.73349\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_residual <- function(Y, Y_hat) {\n  Y - Y_hat\n}\nresidual <- get_residual(Y, Y_hat)\nhead(residual)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]\n1 -7.105427e-15\n2 -2.842171e-14\n3 -3.552714e-15\n4 -1.421085e-14\n5  2.842171e-14\n6  0.000000e+00\n```\n:::\n:::\n\n\n### step 4: 求出系数的标准误\n\n感觉这一部分的可拓展性比较强，在不同的假设下（同方差、异方差等），系数的标准误不同。这次先假设同方差且无自相关（球星扰动项）。\n\n求一下自由度\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_freedom <- function(X) {\n  nrow(X) - ncol(X)\n}\nfreedom <- get_freedom(X)\nfreedom\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 994\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_var_sepherical <- function(residual, freedom) {\n  as.numeric(crossprod(residual, residual) / freedom)\n}\nresidual_var <- get_var_sepherical(residual, freedom)\nresidual_var\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.95194e-28\n```\n:::\n:::\n\n\n球星扰动项假设下，系数的标准误为\n\n$$\n\\mathbf {\\text{SE}(\\hat{\\beta})} = \\mathbf {\\sqrt {\\text{Var}(e) (X^T X)^{-1}_{kk}}}\n$$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_beta_se <- function(X, residual_var) {\n  sqrt(residual_var * diag(solve(crossprod(X, X))))\n}\nbeta_se <- get_beta_se(X, residual_var)\nbeta_se\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n (Intercept)           x1           x2          x3B          x3C          x3D \n2.135352e-15 1.373596e-16 1.354870e-16 2.187586e-15 2.153656e-15 2.223240e-15 \n```\n:::\n:::\n\n\n### step 5: 系数的t检验\n\n计算系数的t统计量\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_beta_t <- function(beta, beta_se, assump = 0) {\n  (beta - assump) / beta_se\n}\nbeta_t <- get_beta_t(beta, beta_se)\nbeta_t\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                    [,1]\n(Intercept) 9.366136e+14\nx1          1.456032e+16\nx2          2.214234e+16\nx3B         4.571248e+14\nx3C         9.286532e+14\nx3D         1.349382e+15\n```\n:::\n:::\n\n\n置信区间\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_conf <- function(beta, beta_se, freedom, p = 0.05) {\n  beta_lconf <- beta + qt(p, freedom) * beta_se\n  beta_hconf <- beta - qt(p, freedom) * beta_se\n  cbind(beta_lconf, beta_hconf)\n}\n\nconf <- get_conf(beta, beta_se, freedom)\nconf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            [,1] [,2]\n(Intercept)    2    2\nx1             2    2\nx2             3    3\nx3B            1    1\nx3C            2    2\nx3D            3    3\n```\n:::\n:::\n\n\n\np-value\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_pvalue <- function(beta_t, freedom) {\n  1 - pt(abs(beta_t), freedom)\n}\npvalue <- get_pvalue(beta_t, freedom)\npvalue\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            [,1]\n(Intercept)    0\nx1             0\nx2             0\nx3B            0\nx3C            0\nx3D            0\n```\n:::\n:::\n\n\n\n### step 6: 整理结果\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresult_df <- as_tibble(beta, rownames = \"term\") %>% \n  rename(estimate = V1) %>% \n  bind_cols(std_error = beta_se,\n            statistics = beta_t[, 1],\n            p_value = pvalue[, 1])\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n```\n:::\n:::\n\n\n\n# 打包成一个函数\n\n前面编写的函数有\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_beta <- function(X, Y) {\n  solve(crossprod(X, X)) %*% crossprod(X, Y)\n}\n\nget_fit <- function(X, beta) {\n  X %*% beta\n}\n\nget_residual <- function(Y, Y_hat) {\n  Y - Y_hat\n}\n\nget_freedom <- function(X) {\n  nrow(X) - ncol(X)\n}\n\nget_var_sepherical <- function(residual, freedom) {\n  as.numeric(crossprod(residual, residual) / freedom)\n}\n\nget_beta_se <- function(X, residual) {\n  sqrt(residual * diag(solve(crossprod(X, X))))\n}\n\nget_beta_t <- function(beta, beta_se, assump = 0) {\n  (beta - assump) / beta_se\n}\n\nget_conf <- function(beta, beta_se, freedom, p = 0.05) {\n  beta_lconf <- beta + qt(p, freedom) * beta_se\n  beta_hconf <- beta - qt(p, freedom) * beta_se\n  cbind(beta_lconf, beta_hconf)\n}\n\nget_pvalue <- function(beta_t, freedom) {\n  1 - pt(abs(beta_t), freedom)\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmy_lm <- function(formula, df) {\n  df_model <- model.frame(formula, df)\n  X <- model.matrix.lm(df_model)\n  Y <- model.response(df_model)\n  \n  beta <- get_beta(X, Y)\n  \n  Y_hat <- get_fit(X, beta)\n  residual <- get_residual(Y, Y_hat)\n  \n  freedom <- get_freedom(X)\n  residual_var <- get_var_sepherical(residual, freedom)\n  \n  beta_se <- get_beta_se(X, residual_var)\n  \n  beta_t <- get_beta_t(beta, beta_se)\n  #conf <- get_conf(beta, beta_se, freedom)\n  pvalue <- get_pvalue(beta_t, freedom)\n  \n  \n  result_df <- as_tibble(beta,rownames = \"term\") %>% \n    rename(estimate = V1) %>% \n    bind_cols(std_error = beta_se,\n              statistics = beta_t[, 1],\n              p_value = pvalue[, 1])\n  \n  return(result_df)\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmy_result <- my_lm(y ~ x1 + x2 + x3, df = df)\nmy_result\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 5\n  term        estimate std_error statistics p_value\n  <chr>          <dbl>     <dbl>      <dbl>   <dbl>\n1 (Intercept)     2.00  2.14e-15    9.37e14       0\n2 x1              2.00  1.37e-16    1.46e16       0\n3 x2              3     1.35e-16    2.21e16       0\n4 x3B             1     2.19e-15    4.57e14       0\n5 x3C             2.00  2.15e-15    9.29e14       0\n6 x3D             3.00  2.22e-15    1.35e15       0\n```\n:::\n:::\n\n\n对比一下`lm()`的结果\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm_result <- lm(y ~ x1 + x2 + x3, df) %>% \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in summary.lm(x): essentially perfect fit: summary may be unreliable\n```\n:::\n\n```{.r .cell-code}\nlm_result\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)     2     9.24e-16   2.16e15       0\n2 x1              2     5.94e-17   3.37e16       0\n3 x2              3     5.86e-17   5.12e16       0\n4 x3B             1.00  9.47e-16   1.06e15       0\n5 x3C             2.00  9.32e-16   2.15e15       0\n6 x3D             3     9.62e-16   3.12e15       0\n```\n:::\n:::\n\n\n结果看起来不对，但其实是浮点数的数字存储机制造成的，比如\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsqrt(2) ^ 2 == 2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n:::\n\n\n这使应该使用`near()`来判断两个数是否相等\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnear(sqrt(2) ^ 2, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnear(my_result$std_error, lm_result$std.error)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)          x1          x2         x3B         x3C         x3D \n       TRUE        TRUE        TRUE        TRUE        TRUE        TRUE \n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnear(my_result$statistics, lm_result$statistic)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)          x1          x2         x3B         x3C         x3D \n      FALSE       FALSE       FALSE       FALSE       FALSE       FALSE \n```\n:::\n:::\n\n\n# 使用真实的数据集\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nChickWeight\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    weight Time Chick Diet\n1       42    0     1    1\n2       51    2     1    1\n3       59    4     1    1\n4       64    6     1    1\n5       76    8     1    1\n6       93   10     1    1\n7      106   12     1    1\n8      125   14     1    1\n9      149   16     1    1\n10     171   18     1    1\n11     199   20     1    1\n12     205   21     1    1\n13      40    0     2    1\n14      49    2     2    1\n15      58    4     2    1\n16      72    6     2    1\n17      84    8     2    1\n18     103   10     2    1\n19     122   12     2    1\n20     138   14     2    1\n21     162   16     2    1\n22     187   18     2    1\n23     209   20     2    1\n24     215   21     2    1\n25      43    0     3    1\n26      39    2     3    1\n27      55    4     3    1\n28      67    6     3    1\n29      84    8     3    1\n30      99   10     3    1\n31     115   12     3    1\n32     138   14     3    1\n33     163   16     3    1\n34     187   18     3    1\n35     198   20     3    1\n36     202   21     3    1\n37      42    0     4    1\n38      49    2     4    1\n39      56    4     4    1\n40      67    6     4    1\n41      74    8     4    1\n42      87   10     4    1\n43     102   12     4    1\n44     108   14     4    1\n45     136   16     4    1\n46     154   18     4    1\n47     160   20     4    1\n48     157   21     4    1\n49      41    0     5    1\n50      42    2     5    1\n51      48    4     5    1\n52      60    6     5    1\n53      79    8     5    1\n54     106   10     5    1\n55     141   12     5    1\n56     164   14     5    1\n57     197   16     5    1\n58     199   18     5    1\n59     220   20     5    1\n60     223   21     5    1\n61      41    0     6    1\n62      49    2     6    1\n63      59    4     6    1\n64      74    6     6    1\n65      97    8     6    1\n66     124   10     6    1\n67     141   12     6    1\n68     148   14     6    1\n69     155   16     6    1\n70     160   18     6    1\n71     160   20     6    1\n72     157   21     6    1\n73      41    0     7    1\n74      49    2     7    1\n75      57    4     7    1\n76      71    6     7    1\n77      89    8     7    1\n78     112   10     7    1\n79     146   12     7    1\n80     174   14     7    1\n81     218   16     7    1\n82     250   18     7    1\n83     288   20     7    1\n84     305   21     7    1\n85      42    0     8    1\n86      50    2     8    1\n87      61    4     8    1\n88      71    6     8    1\n89      84    8     8    1\n90      93   10     8    1\n91     110   12     8    1\n92     116   14     8    1\n93     126   16     8    1\n94     134   18     8    1\n95     125   20     8    1\n96      42    0     9    1\n97      51    2     9    1\n98      59    4     9    1\n99      68    6     9    1\n100     85    8     9    1\n101     96   10     9    1\n102     90   12     9    1\n103     92   14     9    1\n104     93   16     9    1\n105    100   18     9    1\n106    100   20     9    1\n107     98   21     9    1\n108     41    0    10    1\n109     44    2    10    1\n110     52    4    10    1\n111     63    6    10    1\n112     74    8    10    1\n113     81   10    10    1\n114     89   12    10    1\n115     96   14    10    1\n116    101   16    10    1\n117    112   18    10    1\n118    120   20    10    1\n119    124   21    10    1\n120     43    0    11    1\n121     51    2    11    1\n122     63    4    11    1\n123     84    6    11    1\n124    112    8    11    1\n125    139   10    11    1\n126    168   12    11    1\n127    177   14    11    1\n128    182   16    11    1\n129    184   18    11    1\n130    181   20    11    1\n131    175   21    11    1\n132     41    0    12    1\n133     49    2    12    1\n134     56    4    12    1\n135     62    6    12    1\n136     72    8    12    1\n137     88   10    12    1\n138    119   12    12    1\n139    135   14    12    1\n140    162   16    12    1\n141    185   18    12    1\n142    195   20    12    1\n143    205   21    12    1\n144     41    0    13    1\n145     48    2    13    1\n146     53    4    13    1\n147     60    6    13    1\n148     65    8    13    1\n149     67   10    13    1\n150     71   12    13    1\n151     70   14    13    1\n152     71   16    13    1\n153     81   18    13    1\n154     91   20    13    1\n155     96   21    13    1\n156     41    0    14    1\n157     49    2    14    1\n158     62    4    14    1\n159     79    6    14    1\n160    101    8    14    1\n161    128   10    14    1\n162    164   12    14    1\n163    192   14    14    1\n164    227   16    14    1\n165    248   18    14    1\n166    259   20    14    1\n167    266   21    14    1\n168     41    0    15    1\n169     49    2    15    1\n170     56    4    15    1\n171     64    6    15    1\n172     68    8    15    1\n173     68   10    15    1\n174     67   12    15    1\n175     68   14    15    1\n176     41    0    16    1\n177     45    2    16    1\n178     49    4    16    1\n179     51    6    16    1\n180     57    8    16    1\n181     51   10    16    1\n182     54   12    16    1\n183     42    0    17    1\n184     51    2    17    1\n185     61    4    17    1\n186     72    6    17    1\n187     83    8    17    1\n188     89   10    17    1\n189     98   12    17    1\n190    103   14    17    1\n191    113   16    17    1\n192    123   18    17    1\n193    133   20    17    1\n194    142   21    17    1\n195     39    0    18    1\n196     35    2    18    1\n197     43    0    19    1\n198     48    2    19    1\n199     55    4    19    1\n200     62    6    19    1\n201     65    8    19    1\n202     71   10    19    1\n203     82   12    19    1\n204     88   14    19    1\n205    106   16    19    1\n206    120   18    19    1\n207    144   20    19    1\n208    157   21    19    1\n209     41    0    20    1\n210     47    2    20    1\n211     54    4    20    1\n212     58    6    20    1\n213     65    8    20    1\n214     73   10    20    1\n215     77   12    20    1\n216     89   14    20    1\n217     98   16    20    1\n218    107   18    20    1\n219    115   20    20    1\n220    117   21    20    1\n221     40    0    21    2\n222     50    2    21    2\n223     62    4    21    2\n224     86    6    21    2\n225    125    8    21    2\n226    163   10    21    2\n227    217   12    21    2\n228    240   14    21    2\n229    275   16    21    2\n230    307   18    21    2\n231    318   20    21    2\n232    331   21    21    2\n233     41    0    22    2\n234     55    2    22    2\n235     64    4    22    2\n236     77    6    22    2\n237     90    8    22    2\n238     95   10    22    2\n239    108   12    22    2\n240    111   14    22    2\n241    131   16    22    2\n242    148   18    22    2\n243    164   20    22    2\n244    167   21    22    2\n245     43    0    23    2\n246     52    2    23    2\n247     61    4    23    2\n248     73    6    23    2\n249     90    8    23    2\n250    103   10    23    2\n251    127   12    23    2\n252    135   14    23    2\n253    145   16    23    2\n254    163   18    23    2\n255    170   20    23    2\n256    175   21    23    2\n257     42    0    24    2\n258     52    2    24    2\n259     58    4    24    2\n260     74    6    24    2\n261     66    8    24    2\n262     68   10    24    2\n263     70   12    24    2\n264     71   14    24    2\n265     72   16    24    2\n266     72   18    24    2\n267     76   20    24    2\n268     74   21    24    2\n269     40    0    25    2\n270     49    2    25    2\n271     62    4    25    2\n272     78    6    25    2\n273    102    8    25    2\n274    124   10    25    2\n275    146   12    25    2\n276    164   14    25    2\n277    197   16    25    2\n278    231   18    25    2\n279    259   20    25    2\n280    265   21    25    2\n281     42    0    26    2\n282     48    2    26    2\n283     57    4    26    2\n284     74    6    26    2\n285     93    8    26    2\n286    114   10    26    2\n287    136   12    26    2\n288    147   14    26    2\n289    169   16    26    2\n290    205   18    26    2\n291    236   20    26    2\n292    251   21    26    2\n293     39    0    27    2\n294     46    2    27    2\n295     58    4    27    2\n296     73    6    27    2\n297     87    8    27    2\n298    100   10    27    2\n299    115   12    27    2\n300    123   14    27    2\n301    144   16    27    2\n302    163   18    27    2\n303    185   20    27    2\n304    192   21    27    2\n305     39    0    28    2\n306     46    2    28    2\n307     58    4    28    2\n308     73    6    28    2\n309     92    8    28    2\n310    114   10    28    2\n311    145   12    28    2\n312    156   14    28    2\n313    184   16    28    2\n314    207   18    28    2\n315    212   20    28    2\n316    233   21    28    2\n317     39    0    29    2\n318     48    2    29    2\n319     59    4    29    2\n320     74    6    29    2\n321     87    8    29    2\n322    106   10    29    2\n323    134   12    29    2\n324    150   14    29    2\n325    187   16    29    2\n326    230   18    29    2\n327    279   20    29    2\n328    309   21    29    2\n329     42    0    30    2\n330     48    2    30    2\n331     59    4    30    2\n332     72    6    30    2\n333     85    8    30    2\n334     98   10    30    2\n335    115   12    30    2\n336    122   14    30    2\n337    143   16    30    2\n338    151   18    30    2\n339    157   20    30    2\n340    150   21    30    2\n341     42    0    31    3\n342     53    2    31    3\n343     62    4    31    3\n344     73    6    31    3\n345     85    8    31    3\n346    102   10    31    3\n347    123   12    31    3\n348    138   14    31    3\n349    170   16    31    3\n350    204   18    31    3\n351    235   20    31    3\n352    256   21    31    3\n353     41    0    32    3\n354     49    2    32    3\n355     65    4    32    3\n356     82    6    32    3\n357    107    8    32    3\n358    129   10    32    3\n359    159   12    32    3\n360    179   14    32    3\n361    221   16    32    3\n362    263   18    32    3\n363    291   20    32    3\n364    305   21    32    3\n365     39    0    33    3\n366     50    2    33    3\n367     63    4    33    3\n368     77    6    33    3\n369     96    8    33    3\n370    111   10    33    3\n371    137   12    33    3\n372    144   14    33    3\n373    151   16    33    3\n374    146   18    33    3\n375    156   20    33    3\n376    147   21    33    3\n377     41    0    34    3\n378     49    2    34    3\n379     63    4    34    3\n380     85    6    34    3\n381    107    8    34    3\n382    134   10    34    3\n383    164   12    34    3\n384    186   14    34    3\n385    235   16    34    3\n386    294   18    34    3\n387    327   20    34    3\n388    341   21    34    3\n389     41    0    35    3\n390     53    2    35    3\n391     64    4    35    3\n392     87    6    35    3\n393    123    8    35    3\n394    158   10    35    3\n395    201   12    35    3\n396    238   14    35    3\n397    287   16    35    3\n398    332   18    35    3\n399    361   20    35    3\n400    373   21    35    3\n401     39    0    36    3\n402     48    2    36    3\n403     61    4    36    3\n404     76    6    36    3\n405     98    8    36    3\n406    116   10    36    3\n407    145   12    36    3\n408    166   14    36    3\n409    198   16    36    3\n410    227   18    36    3\n411    225   20    36    3\n412    220   21    36    3\n413     41    0    37    3\n414     48    2    37    3\n415     56    4    37    3\n416     68    6    37    3\n417     80    8    37    3\n418     83   10    37    3\n419    103   12    37    3\n420    112   14    37    3\n421    135   16    37    3\n422    157   18    37    3\n423    169   20    37    3\n424    178   21    37    3\n425     41    0    38    3\n426     49    2    38    3\n427     61    4    38    3\n428     74    6    38    3\n429     98    8    38    3\n430    109   10    38    3\n431    128   12    38    3\n432    154   14    38    3\n433    192   16    38    3\n434    232   18    38    3\n435    280   20    38    3\n436    290   21    38    3\n437     42    0    39    3\n438     50    2    39    3\n439     61    4    39    3\n440     78    6    39    3\n441     89    8    39    3\n442    109   10    39    3\n443    130   12    39    3\n444    146   14    39    3\n445    170   16    39    3\n446    214   18    39    3\n447    250   20    39    3\n448    272   21    39    3\n449     41    0    40    3\n450     55    2    40    3\n451     66    4    40    3\n452     79    6    40    3\n453    101    8    40    3\n454    120   10    40    3\n455    154   12    40    3\n456    182   14    40    3\n457    215   16    40    3\n458    262   18    40    3\n459    295   20    40    3\n460    321   21    40    3\n461     42    0    41    4\n462     51    2    41    4\n463     66    4    41    4\n464     85    6    41    4\n465    103    8    41    4\n466    124   10    41    4\n467    155   12    41    4\n468    153   14    41    4\n469    175   16    41    4\n470    184   18    41    4\n471    199   20    41    4\n472    204   21    41    4\n473     42    0    42    4\n474     49    2    42    4\n475     63    4    42    4\n476     84    6    42    4\n477    103    8    42    4\n478    126   10    42    4\n479    160   12    42    4\n480    174   14    42    4\n481    204   16    42    4\n482    234   18    42    4\n483    269   20    42    4\n484    281   21    42    4\n485     42    0    43    4\n486     55    2    43    4\n487     69    4    43    4\n488     96    6    43    4\n489    131    8    43    4\n490    157   10    43    4\n491    184   12    43    4\n492    188   14    43    4\n493    197   16    43    4\n494    198   18    43    4\n495    199   20    43    4\n496    200   21    43    4\n497     42    0    44    4\n498     51    2    44    4\n499     65    4    44    4\n500     86    6    44    4\n501    103    8    44    4\n502    118   10    44    4\n503    127   12    44    4\n504    138   14    44    4\n505    145   16    44    4\n506    146   18    44    4\n507     41    0    45    4\n508     50    2    45    4\n509     61    4    45    4\n510     78    6    45    4\n511     98    8    45    4\n512    117   10    45    4\n513    135   12    45    4\n514    141   14    45    4\n515    147   16    45    4\n516    174   18    45    4\n517    197   20    45    4\n518    196   21    45    4\n519     40    0    46    4\n520     52    2    46    4\n521     62    4    46    4\n522     82    6    46    4\n523    101    8    46    4\n524    120   10    46    4\n525    144   12    46    4\n526    156   14    46    4\n527    173   16    46    4\n528    210   18    46    4\n529    231   20    46    4\n530    238   21    46    4\n531     41    0    47    4\n532     53    2    47    4\n533     66    4    47    4\n534     79    6    47    4\n535    100    8    47    4\n536    123   10    47    4\n537    148   12    47    4\n538    157   14    47    4\n539    168   16    47    4\n540    185   18    47    4\n541    210   20    47    4\n542    205   21    47    4\n543     39    0    48    4\n544     50    2    48    4\n545     62    4    48    4\n546     80    6    48    4\n547    104    8    48    4\n548    125   10    48    4\n549    154   12    48    4\n550    170   14    48    4\n551    222   16    48    4\n552    261   18    48    4\n553    303   20    48    4\n554    322   21    48    4\n555     40    0    49    4\n556     53    2    49    4\n557     64    4    49    4\n558     85    6    49    4\n559    108    8    49    4\n560    128   10    49    4\n561    152   12    49    4\n562    166   14    49    4\n563    184   16    49    4\n564    203   18    49    4\n565    233   20    49    4\n566    237   21    49    4\n567     41    0    50    4\n568     54    2    50    4\n569     67    4    50    4\n570     84    6    50    4\n571    105    8    50    4\n572    122   10    50    4\n573    155   12    50    4\n574    175   14    50    4\n575    205   16    50    4\n576    234   18    50    4\n577    264   20    50    4\n578    264   21    50    4\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmtcars\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmy_lm(mpg ~ ., mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 11 × 5\n   term        estimate std_error statistics p_value\n   <chr>          <dbl>     <dbl>      <dbl>   <dbl>\n 1 (Intercept)  12.3      18.7         0.657  0.259 \n 2 cyl          -0.111     1.05       -0.107  0.458 \n 3 disp          0.0133    0.0179      0.747  0.232 \n 4 hp           -0.0215    0.0218     -0.987  0.167 \n 5 drat          0.787     1.64        0.481  0.318 \n 6 wt           -3.72      1.89       -1.96   0.0316\n 7 qsec          0.821     0.731       1.12   0.137 \n 8 vs            0.318     2.10        0.151  0.441 \n 9 am            2.52      2.06        1.23   0.117 \n10 gear          0.655     1.49        0.439  0.333 \n11 carb         -0.199     0.829      -0.241  0.406 \n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(mpg ~ ., mtcars) %>% \n  broom::tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 11 × 5\n   term        estimate std.error statistic p.value\n   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)  12.3      18.7        0.657  0.518 \n 2 cyl          -0.111     1.05      -0.107  0.916 \n 3 disp          0.0133    0.0179     0.747  0.463 \n 4 hp           -0.0215    0.0218    -0.987  0.335 \n 5 drat          0.787     1.64       0.481  0.635 \n 6 wt           -3.72      1.89      -1.96   0.0633\n 7 qsec          0.821     0.731      1.12   0.274 \n 8 vs            0.318     2.10       0.151  0.881 \n 9 am            2.52      2.06       1.23   0.234 \n10 gear          0.655     1.49       0.439  0.665 \n11 carb         -0.199     0.829     -0.241  0.812 \n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}